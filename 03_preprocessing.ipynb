{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34135dd4",
   "metadata": {},
   "source": [
    "# ðŸ”§ Data Preprocessing for F1 Probability Modeling\n",
    "## Preparing Singapore GP data for VAE and Bayesian Network implementation\n",
    "\n",
    "This notebook handles:\n",
    "- Feature engineering and selection\n",
    "- Data normalization and scaling\n",
    "- Missing value treatment\n",
    "- Train/validation splits\n",
    "- Feature encoding for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2d0b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Preprocessing libraries loaded successfully!\n",
      "âš–ï¸ Feature weighting system enabled: 15 weighted features\n",
      "ðŸŽ¯ Target circuit: Singapore\n",
      "ðŸ“Š Available circuits: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Import configuration and feature weights\n",
    "from config import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"ðŸ”§ Preprocessing libraries loaded successfully!\")\n",
    "print(f\"âš–ï¸ Feature weighting system enabled: {len(get_weighted_features('all'))} weighted features\")\n",
    "print(f\"ðŸŽ¯ Target circuit: {DATA_CONFIG['selected_circuit']}\")\n",
    "print(f\"ðŸ“Š Available circuits: {len(get_available_circuits())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd3d3ab",
   "metadata": {},
   "source": [
    "    # ðŸ“‚ Load Processed Dataset - Universal Circuit Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe51b141",
   "metadata": {},
   "source": [
    "## âœ… Data Flow Verification\n",
    "\n",
    "Verify that we're correctly receiving data from notebook 02 (data analysis output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd4c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading cleaned dataset from analysis (notebook 02)...\n",
      "âœ… Loading SINGAPORE GP cleaned data from notebook 02: data/processed\\singapore_cleaned_20251026_234118.csv\n",
      "âœ… Data loaded successfully!\n",
      "ðŸ“Š Shape: (240, 21)\n",
      "ðŸ Years: [np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "ðŸŽï¸ Unique drivers: 30\n",
      "ðŸ Unique teams: 13\n",
      "âš–ï¸ Weighted dataset detected (weight range: 1.2 - 3.0)\n",
      "ðŸ”§ Ready for preprocessing...\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset from notebook 02 (data analysis output)\n",
    "print(\"ðŸ“‚ Loading cleaned dataset from analysis (notebook 02)...\")\n",
    "\n",
    "# Priority 1: Load cleaned data from notebook 02\n",
    "cleaned_files = glob.glob('data/processed/*_cleaned_*.csv')\n",
    "\n",
    "if cleaned_files:\n",
    "    latest_file = max(cleaned_files, key=os.path.getctime)\n",
    "    circuit_name = os.path.basename(latest_file).split('_')[0].upper()\n",
    "    print(f\"âœ… Loading {circuit_name} GP cleaned data from notebook 02: {latest_file}\")\n",
    "    df = pd.read_csv(latest_file)\n",
    "    data_source = \"cleaned\"\n",
    "    \n",
    "elif glob.glob('data/raw/*_prediction_weighted_*.csv'):\n",
    "    # Fallback to weighted prediction data if cleaned not available\n",
    "    weighted_files = glob.glob('data/raw/*_prediction_weighted_*.csv')\n",
    "    latest_file = max(weighted_files, key=os.path.getctime)\n",
    "    circuit_name = os.path.basename(latest_file).split('_')[0].upper()\n",
    "    print(f\"âš ï¸ Cleaned data not found, using weighted prediction data: {latest_file}\")\n",
    "    df = pd.read_csv(latest_file)\n",
    "    data_source = \"weighted_prediction\"\n",
    "else:\n",
    "    print(\"âŒ No data files found. Please run notebook 01 and 02 first.\")\n",
    "    df = pd.DataFrame()\n",
    "    data_source = None\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"âœ… Data loaded successfully!\")\n",
    "    print(f\"ðŸ“Š Shape: {df.shape}\")\n",
    "    print(f\"ðŸ Years: {sorted(df['year'].unique())}\")\n",
    "    print(f\"ðŸŽï¸ Unique drivers: {df['driver_name'].nunique()}\")\n",
    "    print(f\"ðŸ Unique teams: {df['team'].nunique()}\")\n",
    "    \n",
    "    # Check for weighted data features\n",
    "    if 'data_weight' in df.columns:\n",
    "        print(f\"âš–ï¸ Weighted dataset detected (weight range: {df['data_weight'].min():.1f} - {df['data_weight'].max():.1f})\")\n",
    "    \n",
    "    print(f\"ðŸ”§ Ready for preprocessing...\")\n",
    "else:\n",
    "    print(\"ðŸš« Cannot proceed without data. Please run notebooks 01 and 02 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d809f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DATA FLOW VERIFICATION FROM NOTEBOOK 02\n",
      "============================================================\n",
      "âœ… All expected base columns present\n",
      "âœ… Weighted data detected (from notebook 01)\n",
      "   Weight range: 1.2 - 3.0\n",
      "ðŸ“Š Data completeness: 100.0%\n",
      "\n",
      "ðŸ“ Data source: cleaned\n",
      "ðŸ“Š Shape: (240, 21)\n",
      "ðŸ Years: [np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "\n",
      "âœ… Data flow from notebook 02 verified successfully!\n",
      "ðŸ”§ Ready to proceed with preprocessing...\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"âœ… DATA FLOW VERIFICATION FROM NOTEBOOK 02\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Verify expected columns from notebook 02\n",
    "    expected_base_columns = [\n",
    "        'year', 'driver_name', 'team', 'grid_pos', 'finish_pos', \n",
    "        'quali_pos', 'points', 'pit_stops', 'pos_change'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in expected_base_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"âš ï¸ Missing expected columns: {missing_columns}\")\n",
    "    else:\n",
    "        print(f\"âœ… All expected base columns present\")\n",
    "    \n",
    "    # Check for data weighting (from notebook 01)\n",
    "    if 'data_weight' in df.columns:\n",
    "        print(f\"âœ… Weighted data detected (from notebook 01)\")\n",
    "        print(f\"   Weight range: {df['data_weight'].min():.1f} - {df['data_weight'].max():.1f}\")\n",
    "    \n",
    "    # Check data quality (from notebook 02 cleaning)\n",
    "    missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100\n",
    "    print(f\"ðŸ“Š Data completeness: {100 - missing_pct:.1f}%\")\n",
    "    \n",
    "    # Verify data source\n",
    "    print(f\"\\nðŸ“ Data source: {data_source}\")\n",
    "    print(f\"ðŸ“Š Shape: {df.shape}\")\n",
    "    print(f\"ðŸ Years: {sorted(df['year'].unique())}\")\n",
    "    \n",
    "    print(\"\\nâœ… Data flow from notebook 02 verified successfully!\")\n",
    "    print(\"ðŸ”§ Ready to proceed with preprocessing...\")\n",
    "else:\n",
    "    print(\"âŒ Data flow verification failed - no data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebb23f",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12edec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Creating rolling features and engineered metrics...\n",
      "  ðŸŽï¸ Computing driver form metrics...\n",
      "  ðŸ Computing team performance metrics...\n",
      "  â›½ Computing strategy metrics...\n",
      "  â±ï¸ Normalizing qualifying times...\n",
      "  ðŸ“š Computing experience metrics...\n",
      "âœ… Created 10 rolling/engineered features\n",
      "ðŸ“‹ Updated dataset shape: (240, 31)\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ðŸ“ˆ Creating rolling features and engineered metrics...\")\n",
    "    \n",
    "    # Create a copy and sort by driver and year\n",
    "    df_features = df.copy().sort_values(['driver_name', 'year'])\n",
    "    \n",
    "    # 1. Driver Form - Rolling averages (last 2 races per driver)\n",
    "    print(\"  ðŸŽï¸ Computing driver form metrics...\")\n",
    "    df_features['driver_quali_form'] = df_features.groupby('driver_name')['quali_pos'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    df_features['driver_race_form'] = df_features.groupby('driver_name')['finish_pos'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    df_features['driver_points_form'] = df_features.groupby('driver_name')['points'].rolling(\n",
    "        window=2, min_periods=1\n",
    "    ).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    # 2. Team Performance\n",
    "    print(\"  ðŸ Computing team performance metrics...\")\n",
    "    team_quali_avg = df_features.groupby(['team', 'year'])['quali_pos'].transform('mean')\n",
    "    df_features['team_quali_avg'] = team_quali_avg\n",
    "    \n",
    "    team_race_avg = df_features.groupby(['team', 'year'])['finish_pos'].transform('mean')\n",
    "    df_features['team_race_avg'] = team_race_avg\n",
    "    \n",
    "    # 3. Strategic Features\n",
    "    print(\"  â›½ Computing strategy metrics...\")\n",
    "    df_features['quali_race_delta'] = df_features['finish_pos'] - df_features['quali_pos']\n",
    "    df_features['abs_pos_change'] = abs(df_features['pos_change'])\n",
    "    \n",
    "    # 4. Normalize qualifying times\n",
    "    print(\"  â±ï¸ Normalizing qualifying times...\")\n",
    "    if 'gap_to_pole' in df_features.columns:\n",
    "        year_max_gap = df_features.groupby('year')['gap_to_pole'].transform('max')\n",
    "        df_features['gap_to_pole_normalized'] = (\n",
    "            df_features['gap_to_pole'] / year_max_gap.replace(0, 1)\n",
    "        ).fillna(0)\n",
    "    \n",
    "    # 5. Experience metrics\n",
    "    print(\"  ðŸ“š Computing experience metrics...\")\n",
    "    df_features['cumulative_races'] = df_features.groupby('driver_name').cumcount() + 1\n",
    "    df_features['years_experience'] = df_features.groupby('driver_name')['year'].transform('nunique')\n",
    "    \n",
    "    print(f\"âœ… Created {len(df_features.columns) - len(df.columns)} rolling/engineered features\")\n",
    "    print(f\"ðŸ“‹ Updated dataset shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44a93df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Creating derived features...\n",
      "  ðŸ† Computing driver skill ratings...\n",
      "  ðŸ Computing team strength ratings...\n",
      "  ðŸ”§ Computing strategy features...\n",
      "  ðŸ“… Computing temporal features...\n",
      "âœ… Created derived features\n",
      "ðŸ”§ Final engineered dataset shape: (240, 36)\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ðŸŽ¯ Creating derived features...\")\n",
    "    \n",
    "    # Ensure we have df_features from previous step\n",
    "    if 'df_features' not in locals():\n",
    "        df_features = df.copy()\n",
    "    \n",
    "    # 1. Driver skill rating\n",
    "    print(\"  ðŸ† Computing driver skill ratings...\")\n",
    "    driver_stats = df_features.groupby('driver_name').agg({\n",
    "        'points': 'mean',\n",
    "        'finish_pos': 'mean',\n",
    "        'pos_change': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Normalize driver skill (0-1 scale)\n",
    "    driver_stats['skill_score'] = (\n",
    "        (driver_stats['points'] / 25) * 0.4 +\n",
    "        ((21 - driver_stats['finish_pos']) / 20) * 0.4 +\n",
    "        ((driver_stats['pos_change'] + 10) / 20) * 0.2\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    df_features['driver_skill'] = df_features['driver_name'].map(driver_stats['skill_score'])\n",
    "    \n",
    "    # 2. Team strength rating\n",
    "    print(\"  ðŸ Computing team strength ratings...\")\n",
    "    team_stats = df_features.groupby('team').agg({\n",
    "        'points': 'mean',\n",
    "        'finish_pos': 'mean'\n",
    "    })\n",
    "    \n",
    "    max_team_points = team_stats['points'].max() if team_stats['points'].max() > 0 else 1\n",
    "    team_stats['strength_score'] = (\n",
    "        (team_stats['points'] / max_team_points) * 0.6 +\n",
    "        ((21 - team_stats['finish_pos']) / 20) * 0.4\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    df_features['team_strength'] = df_features['team'].map(team_stats['strength_score'])\n",
    "    \n",
    "    # 3. Strategy indicators\n",
    "    print(\"  ðŸ”§ Computing strategy features...\")\n",
    "    df_features['is_aggressive_strategy'] = (df_features['pit_stops'] >= 2).astype(int)\n",
    "    \n",
    "    # Race completion rate\n",
    "    max_laps_by_year = df_features.groupby('year')['total_laps'].transform('max')\n",
    "    df_features['race_completion_rate'] = (\n",
    "        df_features['total_laps'] / max_laps_by_year.replace(0, 1)\n",
    "    ).clip(0, 1)\n",
    "    \n",
    "    # 4. Temporal normalization\n",
    "    print(\"  ðŸ“… Computing temporal features...\")\n",
    "    year_range = df_features['year'].max() - df_features['year'].min()\n",
    "    if year_range > 0:\n",
    "        df_features['year_normalized'] = (df_features['year'] - df_features['year'].min()) / year_range\n",
    "    else:\n",
    "        df_features['year_normalized'] = 0.5\n",
    "    \n",
    "    # Fill any NaN values in derived features\n",
    "    derived_cols = ['driver_skill', 'team_strength', 'race_completion_rate', 'year_normalized']\n",
    "    for col in derived_cols:\n",
    "        if col in df_features.columns:\n",
    "            df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    print(f\"âœ… Created derived features\")\n",
    "    print(f\"ðŸ”§ Final engineered dataset shape: {df_features.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for derived features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f430f20",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Data Cleaning & Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c20190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Handling missing values and data cleaning...\n",
      "ðŸ“Š Processing 28 numeric features\n",
      "âœ… Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ðŸ§¹ Handling missing values and data cleaning...\")\n",
    "    \n",
    "    # Get numeric features\n",
    "    numeric_features = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"ðŸ“Š Processing {len(numeric_features)} numeric features\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_summary = df_features[numeric_features].isnull().sum()\n",
    "    if missing_summary.sum() > 0:\n",
    "        print(f\"\\nðŸ” Found {missing_summary.sum()} missing values\")\n",
    "        \n",
    "        # 1. Qualifying times: Fill with median by year\n",
    "        quali_cols = ['q1_time', 'q2_time', 'q3_time', 'gap_to_pole']\n",
    "        for col in quali_cols:\n",
    "            if col in df_features.columns:\n",
    "                df_features.loc[df_features[col] == 0, col] = np.nan\n",
    "                df_features[col] = df_features.groupby('year')[col].transform(\n",
    "                    lambda x: x.fillna(x.median())\n",
    "                )\n",
    "                df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "        \n",
    "        # 2. Position features: Fill with median\n",
    "        position_cols = ['grid_pos', 'finish_pos', 'quali_pos']\n",
    "        for col in position_cols:\n",
    "            if col in df_features.columns:\n",
    "                df_features.loc[df_features[col] == 0, col] = np.nan\n",
    "                df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "        \n",
    "        # 3. Other features: Fill with median\n",
    "        for col in numeric_features:\n",
    "            if col not in quali_cols + position_cols:\n",
    "                df_features[col] = df_features[col].fillna(df_features[col].median())\n",
    "    \n",
    "    # Final check\n",
    "    remaining_missing = df_features[numeric_features].isnull().sum().sum()\n",
    "    print(f\"âœ… Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    if remaining_missing > 0:\n",
    "        print(\"ðŸ”§ Applying final imputation...\")\n",
    "        imputer = KNNImputer(n_neighbors=3)\n",
    "        df_features[numeric_features] = imputer.fit_transform(df_features[numeric_features])\n",
    "        print(\"âœ… Imputation completed\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e9e3f",
   "metadata": {},
   "source": [
    "## ðŸ“ Feature Selection & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2802a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Feature selection and scaling with importance weighting...\n",
      "ðŸ“Š Total numeric columns: 28\n",
      "ðŸ“Š Available features (after exclusions): 25\n",
      "ðŸŽ¯ Target variable: finish_pos\n",
      "\n",
      "âš–ï¸ Feature importance:\n",
      "  â€¢ High (â‰¥0.80): 7 features\n",
      "  â€¢ Medium (0.60-0.80): 13 features\n",
      "  â€¢ Low (0.50-0.60): 1 features\n",
      "  â€¢ Total weighted: 25 features\n",
      "\n",
      "ðŸ† Top 15 weighted features:\n",
      "  â€¢ grid_pos                      : 0.95\n",
      "  â€¢ quali_pos                     : 0.90\n",
      "  â€¢ driver_quali_form             : 0.90\n",
      "  â€¢ team_quali_avg                : 0.90\n",
      "  â€¢ quali_race_delta              : 0.90\n",
      "  â€¢ team_strength                 : 0.85\n",
      "  â€¢ driver_skill                  : 0.80\n",
      "  â€¢ gap_to_pole                   : 0.75\n",
      "  â€¢ data_weight                   : 0.75\n",
      "  â€¢ driver_race_form              : 0.75\n",
      "  â€¢ driver_points_form            : 0.75\n",
      "  â€¢ team_race_avg                 : 0.75\n",
      "  â€¢ cumulative_races              : 0.75\n",
      "  â€¢ race_completion_rate          : 0.75\n",
      "  â€¢ gap_to_pole_normalized        : 0.70\n",
      "\n",
      "ðŸ“¦ Feature set sizes:\n",
      "  â€¢ Core: 7 features\n",
      "  â€¢ VAE: 21 features (all available weighted features)\n",
      "\n",
      "ðŸ”§ Preparing core_weighted dataset...\n",
      "  ðŸ“Š Using 7 features\n",
      "  âœ… Shape: X(240, 7), y(240,)\n",
      "\n",
      "ðŸ”§ Preparing vae_optimized dataset...\n",
      "  ðŸ“Š Using 21 features\n",
      "  âœ… Shape: X(240, 21), y(240,)\n",
      "\n",
      "âœ… Created 2 datasets: ['core_weighted', 'vae_optimized']\n",
      "\n",
      "ðŸ“Š Total numeric columns: 28\n",
      "ðŸ“Š Available features (after exclusions): 25\n",
      "ðŸŽ¯ Target variable: finish_pos\n",
      "\n",
      "âš–ï¸ Feature importance:\n",
      "  â€¢ High (â‰¥0.80): 7 features\n",
      "  â€¢ Medium (0.60-0.80): 13 features\n",
      "  â€¢ Low (0.50-0.60): 1 features\n",
      "  â€¢ Total weighted: 25 features\n",
      "\n",
      "ðŸ† Top 15 weighted features:\n",
      "  â€¢ grid_pos                      : 0.95\n",
      "  â€¢ quali_pos                     : 0.90\n",
      "  â€¢ driver_quali_form             : 0.90\n",
      "  â€¢ team_quali_avg                : 0.90\n",
      "  â€¢ quali_race_delta              : 0.90\n",
      "  â€¢ team_strength                 : 0.85\n",
      "  â€¢ driver_skill                  : 0.80\n",
      "  â€¢ gap_to_pole                   : 0.75\n",
      "  â€¢ data_weight                   : 0.75\n",
      "  â€¢ driver_race_form              : 0.75\n",
      "  â€¢ driver_points_form            : 0.75\n",
      "  â€¢ team_race_avg                 : 0.75\n",
      "  â€¢ cumulative_races              : 0.75\n",
      "  â€¢ race_completion_rate          : 0.75\n",
      "  â€¢ gap_to_pole_normalized        : 0.70\n",
      "\n",
      "ðŸ“¦ Feature set sizes:\n",
      "  â€¢ Core: 7 features\n",
      "  â€¢ VAE: 21 features (all available weighted features)\n",
      "\n",
      "ðŸ”§ Preparing core_weighted dataset...\n",
      "  ðŸ“Š Using 7 features\n",
      "  âœ… Shape: X(240, 7), y(240,)\n",
      "\n",
      "ðŸ”§ Preparing vae_optimized dataset...\n",
      "  ðŸ“Š Using 21 features\n",
      "  âœ… Shape: X(240, 21), y(240,)\n",
      "\n",
      "âœ… Created 2 datasets: ['core_weighted', 'vae_optimized']\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ðŸ“ Feature selection and scaling with importance weighting...\")\n",
    "    \n",
    "    # Get numeric columns only\n",
    "    numeric_columns = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    target = 'finish_pos'\n",
    "    \n",
    "    # Remove target and non-predictive columns (keep encoded features)\n",
    "    exclude_cols = [target, 'year', 'points', 'finish_pos_binned_encoded']  # Exclude target-derived feature\n",
    "    available_features = [col for col in numeric_columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"ðŸ“Š Total numeric columns: {len(numeric_columns)}\")\n",
    "    print(f\"ðŸ“Š Available features (after exclusions): {len(available_features)}\")\n",
    "    print(f\"ðŸŽ¯ Target variable: {target}\")\n",
    "    \n",
    "    # Get weighted features from config\n",
    "    config_weighted = get_weighted_features(\"all\")\n",
    "    \n",
    "    # Map config features to available columns with comprehensive weighting\n",
    "    feature_weights = {}\n",
    "    for col in available_features:\n",
    "        # Check if feature is in config\n",
    "        if col in config_weighted:\n",
    "            feature_weights[col] = config_weighted[col]\n",
    "        # Add weights for engineered features (COMPREHENSIVE)\n",
    "        elif 'driver_skill' in col or 'team_strength' in col:\n",
    "            feature_weights[col] = 0.85\n",
    "        elif 'grid' in col.lower() or 'quali' in col.lower():\n",
    "            feature_weights[col] = 0.90\n",
    "        elif 'form' in col.lower() or 'race' in col.lower():\n",
    "            feature_weights[col] = 0.75  # Rolling features\n",
    "        elif 'gap' in col.lower() or 'time' in col.lower():\n",
    "            feature_weights[col] = 0.70  # Timing features\n",
    "        elif 'pit' in col.lower() or 'strategy' in col.lower():\n",
    "            feature_weights[col] = 0.65  # Strategy features\n",
    "        elif 'experience' in col.lower() or 'cumulative' in col.lower():\n",
    "            feature_weights[col] = 0.60  # Experience features\n",
    "        elif 'normalized' in col.lower() or 'completion' in col.lower():\n",
    "            feature_weights[col] = 0.60  # Normalized features\n",
    "        elif 'encoded' in col.lower():\n",
    "            feature_weights[col] = 0.70  # Categorical encodings\n",
    "        elif 'weight' in col.lower():\n",
    "            feature_weights[col] = 0.75  # Data weighting\n",
    "        elif 'delta' in col.lower() or 'change' in col.lower():\n",
    "            feature_weights[col] = 0.65  # Change metrics\n",
    "        elif 'laps' in col.lower() or 'pos' in col.lower():\n",
    "            feature_weights[col] = 0.60  # Position/lap features\n",
    "        else:\n",
    "            feature_weights[col] = 0.55  # Default weight (included in medium range)\n",
    "    \n",
    "    # Create feature sets by importance (more inclusive thresholds)\n",
    "    high_importance = {k: v for k, v in feature_weights.items() if v >= 0.80}\n",
    "    medium_importance = {k: v for k, v in feature_weights.items() if 0.60 <= v < 0.80}\n",
    "    low_importance = {k: v for k, v in feature_weights.items() if 0.50 <= v < 0.60}\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ Feature importance:\")\n",
    "    print(f\"  â€¢ High (â‰¥0.80): {len(high_importance)} features\")\n",
    "    print(f\"  â€¢ Medium (0.60-0.80): {len(medium_importance)} features\")\n",
    "    print(f\"  â€¢ Low (0.50-0.60): {len(low_importance)} features\")\n",
    "    print(f\"  â€¢ Total weighted: {len(feature_weights)} features\")\n",
    "    \n",
    "    # Show top weighted features\n",
    "    top_features = dict(sorted(feature_weights.items(), key=lambda x: x[1], reverse=True)[:15])\n",
    "    print(f\"\\nðŸ† Top 15 weighted features:\")\n",
    "    for feat, weight in top_features.items():\n",
    "        print(f\"  â€¢ {feat:30}: {weight:.2f}\")\n",
    "    \n",
    "    # Create feature sets\n",
    "    core_features = list(high_importance.keys())[:10]  # Top 10 high-importance\n",
    "    \n",
    "    # VAE features: Use ALL weighted features (high + medium + low)\n",
    "    vae_features = (list(high_importance.keys()) + \n",
    "                   list(medium_importance.keys()) + \n",
    "                   list(low_importance.keys()))\n",
    "    vae_features = list(dict.fromkeys(vae_features))  # Remove duplicates, keep all available\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Feature set sizes:\")\n",
    "    print(f\"  â€¢ Core: {len(core_features)} features\")\n",
    "    print(f\"  â€¢ VAE: {len(vae_features)} features (all available weighted features)\")\n",
    "    \n",
    "    feature_sets = {\n",
    "        'core_weighted': {\n",
    "            'features': core_features,\n",
    "            'weights': {f: feature_weights[f] for f in core_features}\n",
    "        },\n",
    "        'vae_optimized': {\n",
    "            'features': vae_features,\n",
    "            'weights': {f: feature_weights[f] for f in vae_features}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Prepare datasets\n",
    "    datasets = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    for set_name, fset in feature_sets.items():\n",
    "        features = fset['features']\n",
    "        weights = fset['weights']\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Ensure features exist\n",
    "        existing_features = [f for f in features if f in df_features.columns]\n",
    "        if len(existing_features) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nðŸ”§ Preparing {set_name} dataset...\")\n",
    "        print(f\"  ðŸ“Š Using {len(existing_features)} features\")\n",
    "        \n",
    "        # Extract features and target\n",
    "        X = df_features[existing_features].copy()\n",
    "        y = df_features[target].copy()\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        # Apply feature weights\n",
    "        feature_weights_array = np.array([weights.get(col, 0.5) for col in X_scaled.columns])\n",
    "        X_weighted = X_scaled * feature_weights_array\n",
    "        \n",
    "        # Store dataset\n",
    "        datasets[set_name] = {\n",
    "            'X': X_weighted,\n",
    "            'X_scaled': X_scaled,\n",
    "            'X_raw': X,\n",
    "            'y': y,\n",
    "            'features': existing_features,\n",
    "            'weights': {f: weights[f] for f in existing_features}\n",
    "        }\n",
    "        scalers[set_name] = scaler\n",
    "        \n",
    "        print(f\"  âœ… Shape: X{X_weighted.shape}, y{y.shape}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Created {len(datasets)} datasets: {list(datasets.keys())}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for feature selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434dda54",
   "metadata": {},
   "source": [
    "## ðŸŽ² Filter Invalid Drivers & Create Unbiased Train/Validation Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af97115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ² Filtering data and creating unbiased train/validation splits...\n",
      "\n",
      "ðŸ” Step 1: Filtering invalid drivers for current race...\n",
      "   Current race year: 2025\n",
      "   Valid drivers in current year: 21\n",
      "   Drivers: ['Alexander Albon', 'Carlos Sainz', 'Charles Leclerc', 'Esteban Ocon', 'Fernando Alonso', 'Franco Colapinto', 'Gabriel Bortoleto', 'George Russell', 'Isack Hadjar', 'Jack Doohan', 'Kimi Antonelli', 'Lance Stroll', 'Lando Norris', 'Lewis Hamilton', 'Liam Lawson', 'Max Verstappen', 'Nico Hulkenberg', 'Oliver Bearman', 'Oscar Piastri', 'Pierre Gasly', 'Yuki Tsunoda']\n",
      "   âœ… Filtered dataset: 240 â†’ 222 rows (18 removed)\n",
      "\n",
      "ðŸ”„ Processing core_weighted dataset...\n",
      "   Before filter: 240 samples\n",
      "   After filter: 222 samples (18 removed)\n",
      "\n",
      "   ðŸŽ¯ Creating unbiased stratified split...\n",
      "   âœ… Stratified split successful\n",
      "   ðŸ“Š Train: 177 samples (79.7%)\n",
      "   ðŸ“Š Val:   45 samples (20.3%)\n",
      "\n",
      "   ðŸ“… Year distribution in train set:\n",
      "      2022: 9 samples (5.1%)\n",
      "      2023: 12 samples (6.8%)\n",
      "      2024: 12 samples (6.8%)\n",
      "      2025: 144 samples (81.4%)\n",
      "   ðŸ“… Year distribution in val set:\n",
      "      2022: 3 samples (6.7%)\n",
      "      2023: 3 samples (6.7%)\n",
      "      2024: 3 samples (6.7%)\n",
      "      2025: 36 samples (80.0%)\n",
      "\n",
      "   ðŸ Position distribution:\n",
      "      Train - Mean: 10.25, Std: 5.88\n",
      "      Val   - Mean: 10.16, Std: 5.56\n",
      "\n",
      "ðŸ”„ Processing vae_optimized dataset...\n",
      "   Before filter: 240 samples\n",
      "   After filter: 222 samples (18 removed)\n",
      "\n",
      "   ðŸŽ¯ Creating unbiased stratified split...\n",
      "   âœ… Stratified split successful\n",
      "   ðŸ“Š Train: 177 samples (79.7%)\n",
      "   ðŸ“Š Val:   45 samples (20.3%)\n",
      "\n",
      "   ðŸ“… Year distribution in train set:\n",
      "      2022: 9 samples (5.1%)\n",
      "      2023: 12 samples (6.8%)\n",
      "      2024: 12 samples (6.8%)\n",
      "      2025: 144 samples (81.4%)\n",
      "   ðŸ“… Year distribution in val set:\n",
      "      2022: 3 samples (6.7%)\n",
      "      2023: 3 samples (6.7%)\n",
      "      2024: 3 samples (6.7%)\n",
      "      2025: 36 samples (80.0%)\n",
      "\n",
      "   ðŸ Position distribution:\n",
      "      Train - Mean: 10.25, Std: 5.88\n",
      "      Val   - Mean: 10.16, Std: 5.56\n",
      "\n",
      "======================================================================\n",
      "âœ… UNBIASED SPLIT COMPLETE!\n",
      "======================================================================\n",
      "âœ… Filtered to valid drivers only (21 drivers)\n",
      "âœ… Created unbiased stratified splits (80-20)\n",
      "âœ… No temporal bias - all years distributed across train/val\n",
      "âœ… Position-balanced - similar distributions in train/val\n",
      "   âœ… Stratified split successful\n",
      "   ðŸ“Š Train: 177 samples (79.7%)\n",
      "   ðŸ“Š Val:   45 samples (20.3%)\n",
      "\n",
      "   ðŸ“… Year distribution in train set:\n",
      "      2022: 9 samples (5.1%)\n",
      "      2023: 12 samples (6.8%)\n",
      "      2024: 12 samples (6.8%)\n",
      "      2025: 144 samples (81.4%)\n",
      "   ðŸ“… Year distribution in val set:\n",
      "      2022: 3 samples (6.7%)\n",
      "      2023: 3 samples (6.7%)\n",
      "      2024: 3 samples (6.7%)\n",
      "      2025: 36 samples (80.0%)\n",
      "\n",
      "   ðŸ Position distribution:\n",
      "      Train - Mean: 10.25, Std: 5.88\n",
      "      Val   - Mean: 10.16, Std: 5.56\n",
      "\n",
      "ðŸ”„ Processing vae_optimized dataset...\n",
      "   Before filter: 240 samples\n",
      "   After filter: 222 samples (18 removed)\n",
      "\n",
      "   ðŸŽ¯ Creating unbiased stratified split...\n",
      "   âœ… Stratified split successful\n",
      "   ðŸ“Š Train: 177 samples (79.7%)\n",
      "   ðŸ“Š Val:   45 samples (20.3%)\n",
      "\n",
      "   ðŸ“… Year distribution in train set:\n",
      "      2022: 9 samples (5.1%)\n",
      "      2023: 12 samples (6.8%)\n",
      "      2024: 12 samples (6.8%)\n",
      "      2025: 144 samples (81.4%)\n",
      "   ðŸ“… Year distribution in val set:\n",
      "      2022: 3 samples (6.7%)\n",
      "      2023: 3 samples (6.7%)\n",
      "      2024: 3 samples (6.7%)\n",
      "      2025: 36 samples (80.0%)\n",
      "\n",
      "   ðŸ Position distribution:\n",
      "      Train - Mean: 10.25, Std: 5.88\n",
      "      Val   - Mean: 10.16, Std: 5.56\n",
      "\n",
      "======================================================================\n",
      "âœ… UNBIASED SPLIT COMPLETE!\n",
      "======================================================================\n",
      "âœ… Filtered to valid drivers only (21 drivers)\n",
      "âœ… Created unbiased stratified splits (80-20)\n",
      "âœ… No temporal bias - all years distributed across train/val\n",
      "âœ… Position-balanced - similar distributions in train/val\n"
     ]
    }
   ],
   "source": [
    "if not df.empty and 'datasets' in locals() and datasets:\n",
    "    print(\"ðŸŽ² Filtering data and creating unbiased train/validation splits...\")\n",
    "    \n",
    "    # Step 1: Identify current race year and valid drivers\n",
    "    print(\"\\nðŸ” Step 1: Filtering invalid drivers for current race...\")\n",
    "    current_year = df_features['year'].max()\n",
    "    print(f\"   Current race year: {current_year}\")\n",
    "    \n",
    "    # Get drivers who participated in the current year\n",
    "    current_year_drivers = set(df_features[df_features['year'] == current_year]['driver_name'].unique())\n",
    "    print(f\"   Valid drivers in current year: {len(current_year_drivers)}\")\n",
    "    print(f\"   Drivers: {sorted(current_year_drivers)}\")\n",
    "    \n",
    "    # Filter df_features to only include valid drivers\n",
    "    df_features_filtered = df_features[df_features['driver_name'].isin(current_year_drivers)].copy()\n",
    "    rows_before = len(df_features)\n",
    "    rows_after = len(df_features_filtered)\n",
    "    print(f\"   âœ… Filtered dataset: {rows_before} â†’ {rows_after} rows ({rows_before - rows_after} removed)\")\n",
    "    \n",
    "    # Update datasets with filtered indices\n",
    "    splits = {}\n",
    "    \n",
    "    for set_name, dataset in datasets.items():\n",
    "        print(f\"\\nðŸ”„ Processing {set_name} dataset...\")\n",
    "        \n",
    "        X = dataset['X']\n",
    "        y = dataset['y']\n",
    "        \n",
    "        # Filter to only include valid driver indices\n",
    "        valid_indices = [idx for idx in X.index if idx in df_features_filtered.index]\n",
    "        X_filtered = X.loc[valid_indices]\n",
    "        y_filtered = y.loc[valid_indices]\n",
    "        \n",
    "        print(f\"   Before filter: {X.shape[0]} samples\")\n",
    "        print(f\"   After filter: {X_filtered.shape[0]} samples ({X.shape[0] - X_filtered.shape[0]} removed)\")\n",
    "        \n",
    "        if len(X_filtered) < 10:\n",
    "            print(f\"   âš ï¸ Insufficient data after filtering ({len(X_filtered)} samples)\")\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Create UNBIASED stratified split (NO temporal bias)\n",
    "        print(f\"\\n   ðŸŽ¯ Creating unbiased stratified split...\")\n",
    "        \n",
    "        # Stratify by finish position to ensure balanced train/val sets\n",
    "        y_binned = pd.cut(y_filtered, bins=5, labels=False)\n",
    "        \n",
    "        try:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_filtered, y_filtered, \n",
    "                test_size=0.2,  # 80-20 split\n",
    "                stratify=y_binned, \n",
    "                random_state=42,\n",
    "                shuffle=True  # Ensure random shuffling\n",
    "            )\n",
    "            print(f\"   âœ… Stratified split successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Stratification failed ({e}), using random split...\")\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_filtered, y_filtered, \n",
    "                test_size=0.2, \n",
    "                random_state=42,\n",
    "                shuffle=True\n",
    "            )\n",
    "        \n",
    "        print(f\"   ðŸ“Š Train: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_filtered)*100:.1f}%)\")\n",
    "        print(f\"   ðŸ“Š Val:   {X_val.shape[0]} samples ({X_val.shape[0]/len(X_filtered)*100:.1f}%)\")\n",
    "        \n",
    "        # Verify year distribution (should be balanced, not temporal)\n",
    "        train_years = df_features_filtered.loc[X_train.index, 'year']\n",
    "        val_years = df_features_filtered.loc[X_val.index, 'year']\n",
    "        \n",
    "        print(f\"\\n   ðŸ“… Year distribution in train set:\")\n",
    "        train_year_counts = train_years.value_counts().sort_index()\n",
    "        for year, count in train_year_counts.items():\n",
    "            print(f\"      {year}: {count} samples ({count/len(train_years)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"   ðŸ“… Year distribution in val set:\")\n",
    "        val_year_counts = val_years.value_counts().sort_index()\n",
    "        for year, count in val_year_counts.items():\n",
    "            print(f\"      {year}: {count} samples ({count/len(val_years)*100:.1f}%)\")\n",
    "        \n",
    "        # Verify position distribution\n",
    "        print(f\"\\n   ðŸ Position distribution:\")\n",
    "        print(f\"      Train - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "        print(f\"      Val   - Mean: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
    "        \n",
    "        # Store splits (only stratified, no temporal)\n",
    "        splits[set_name] = {\n",
    "            'stratified': {\n",
    "                'X_train': X_train,\n",
    "                'X_val': X_val,\n",
    "                'y_train': y_train,\n",
    "                'y_val': y_val\n",
    "            },\n",
    "            'temporal': {  # Keep structure for backward compatibility but set to None\n",
    "                'X_train': None,\n",
    "                'X_val': None,\n",
    "                'y_train': None,\n",
    "                'y_val': None\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… UNBIASED SPLIT COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"âœ… Filtered to valid drivers only ({len(current_year_drivers)} drivers)\")\n",
    "    print(f\"âœ… Created unbiased stratified splits (80-20)\")\n",
    "    print(f\"âœ… No temporal bias - all years distributed across train/val\")\n",
    "    print(f\"âœ… Position-balanced - similar distributions in train/val\")\n",
    "else:\n",
    "    print(\"âš ï¸ No datasets available for splitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05ef25",
   "metadata": {},
   "source": [
    "## ðŸ“ Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c943a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Encoding categorical features...\n",
      "  ðŸŽï¸ Encoded 30 drivers\n",
      "  ðŸ Encoded 13 teams\n",
      "  ðŸ“… Encoded 4 years\n",
      "  ðŸŽ¯ Created 4 position categories\n",
      "\n",
      "âœ… Categorical encoding complete!\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    print(\"ðŸ“ Encoding categorical features...\")\n",
    "    \n",
    "    categorical_encodings = {}\n",
    "    \n",
    "    # Driver encoding\n",
    "    if 'driver_name' in df_features.columns:\n",
    "        driver_encoder = LabelEncoder()\n",
    "        df_features['driver_encoded'] = driver_encoder.fit_transform(df_features['driver_name'])\n",
    "        categorical_encodings['driver'] = {\n",
    "            'encoder': driver_encoder,\n",
    "            'classes': driver_encoder.classes_,\n",
    "            'n_classes': len(driver_encoder.classes_)\n",
    "        }\n",
    "        print(f\"  ðŸŽï¸ Encoded {len(driver_encoder.classes_)} drivers\")\n",
    "    \n",
    "    # Team encoding\n",
    "    if 'team' in df_features.columns:\n",
    "        team_encoder = LabelEncoder()\n",
    "        df_features['team_encoded'] = team_encoder.fit_transform(df_features['team'])\n",
    "        categorical_encodings['team'] = {\n",
    "            'encoder': team_encoder,\n",
    "            'classes': team_encoder.classes_,\n",
    "            'n_classes': len(team_encoder.classes_)\n",
    "        }\n",
    "        print(f\"  ðŸ Encoded {len(team_encoder.classes_)} teams\")\n",
    "    \n",
    "    # Year encoding\n",
    "    year_encoder = LabelEncoder()\n",
    "    df_features['year_encoded'] = year_encoder.fit_transform(df_features['year'])\n",
    "    categorical_encodings['year'] = {\n",
    "        'encoder': year_encoder,\n",
    "        'classes': year_encoder.classes_,\n",
    "        'n_classes': len(year_encoder.classes_)\n",
    "    }\n",
    "    print(f\"  ðŸ“… Encoded {len(year_encoder.classes_)} years\")\n",
    "    \n",
    "    # Finish position binning for Bayesian Network\n",
    "    position_bins = [0, 5, 10, 15, 21]\n",
    "    position_labels = ['Podium', 'Points', 'Midfield', 'Backmarker']\n",
    "    \n",
    "    df_features['finish_pos_binned'] = pd.cut(\n",
    "        df_features['finish_pos'], \n",
    "        bins=position_bins, \n",
    "        labels=position_labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    pos_bin_encoder = LabelEncoder()\n",
    "    df_features['finish_pos_binned_encoded'] = pos_bin_encoder.fit_transform(df_features['finish_pos_binned'])\n",
    "    categorical_encodings['finish_pos_binned'] = {\n",
    "        'encoder': pos_bin_encoder,\n",
    "        'classes': pos_bin_encoder.classes_,\n",
    "        'n_classes': len(pos_bin_encoder.classes_),\n",
    "        'bins': position_bins,\n",
    "        'labels': position_labels\n",
    "    }\n",
    "    \n",
    "    print(f\"  ðŸŽ¯ Created {len(position_labels)} position categories\")\n",
    "    print(f\"\\nâœ… Categorical encoding complete!\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for encoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79e2b3",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1b286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving preprocessed data and artifacts...\n",
      "âœ… Full engineered dataset: data/preprocessed/singapore_engineered_20251026_234139.csv\n",
      "   ðŸ“Š Shape: (240, 41)\n",
      "   ðŸŽ¯ This file contains ALL features for VAE and BN models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing artifacts: data/preprocessed/preprocessing_artifacts_singapore_20251026_234139.pkl\n",
      "  ðŸ’¾ core_weighted dataset saved\n",
      "  ðŸ’¾ vae_optimized dataset saved\n",
      "âœ… Summary: data/preprocessed/preprocessing_summary_singapore_20251026_234139.json\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ‰ PREPROCESSING COMPLETE!\n",
      "======================================================================\n",
      "ðŸ Circuit: SINGAPORE GP\n",
      "ðŸ“Š Original: (240, 21) â†’ Engineered: (240, 41)\n",
      "â­ Features added: 20\n",
      "ðŸŽ¯ Total features: 41\n",
      "\n",
      "ðŸ“¦ Feature Sets Created:\n",
      "  â€¢ core_weighted: 7 features, 240 samples\n",
      "  â€¢ vae_optimized: 21 features, 240 samples\n",
      "\n",
      "ðŸ”— NEXT STEPS:\n",
      "  1. Notebook 04: Train VAE using engineered dataset\n",
      "  2. Notebook 05: Train Bayesian Network with VAE latent features\n",
      "  3. Both notebooks will read from: data/preprocessed/singapore_engineered_20251026_234139.csv\n",
      "\n",
      "ðŸ“ Key Output Files:\n",
      "  ðŸŽ¯ Main Dataset: data/preprocessed/singapore_engineered_20251026_234139.csv\n",
      "  ðŸ”§ Artifacts: data/preprocessed/preprocessing_artifacts_singapore_20251026_234139.pkl\n",
      "  ðŸ“‹ Summary: data/preprocessed/preprocessing_summary_singapore_20251026_234139.json\n",
      "\n",
      "âœ… Ready for VAE and Bayesian Network implementation!\n"
     ]
    }
   ],
   "source": [
    "if not df.empty and 'datasets' in locals() and datasets:\n",
    "    print(\"ðŸ’¾ Saving preprocessed data and artifacts...\")\n",
    "    \n",
    "    os.makedirs('data/preprocessed', exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Determine circuit name\n",
    "    circuit_name = circuit_name.lower() if 'circuit_name' in locals() else \"singapore\"\n",
    "    \n",
    "    # 1. Save complete engineered dataset (MAIN OUTPUT - Used by notebooks 04 & 05)\n",
    "    full_dataset_path = f\"data/preprocessed/{circuit_name}_engineered_{timestamp}.csv\"\n",
    "    df_features.to_csv(full_dataset_path, index=False)\n",
    "    print(f\"âœ… Full engineered dataset: {full_dataset_path}\")\n",
    "    print(f\"   ðŸ“Š Shape: {df_features.shape}\")\n",
    "    print(f\"   ðŸŽ¯ This file contains ALL features for VAE and BN models\")\n",
    "    \n",
    "    # 2. Save preprocessing artifacts (scalers, encoders, splits)\n",
    "    preprocessing_artifacts = {\n",
    "        'datasets': datasets,\n",
    "        'splits': splits if 'splits' in locals() else {},\n",
    "        'scalers': scalers,\n",
    "        'categorical_encodings': categorical_encodings if 'categorical_encodings' in locals() else {},\n",
    "        'feature_sets': feature_sets if 'feature_sets' in locals() else {},\n",
    "        'circuit_name': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'original_shape': df.shape,\n",
    "        'engineered_shape': df_features.shape\n",
    "    }\n",
    "    \n",
    "    artifacts_path = f\"data/preprocessed/preprocessing_artifacts_{circuit_name}_{timestamp}.pkl\"\n",
    "    with open(artifacts_path, 'wb') as f:\n",
    "        pickle.dump(preprocessing_artifacts, f)\n",
    "    print(f\"âœ… Preprocessing artifacts: {artifacts_path}\")\n",
    "    \n",
    "    # 3. Save individual feature sets (for model-specific usage)\n",
    "    for set_name, dataset in datasets.items():\n",
    "        dataset['X'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_X_{timestamp}.csv\")\n",
    "        dataset['y'].to_csv(f\"data/preprocessed/{circuit_name}_{set_name}_y_{timestamp}.csv\")\n",
    "        print(f\"  ðŸ’¾ {set_name} dataset saved\")\n",
    "    \n",
    "    # 4. Save summary with feature list\n",
    "    summary = {\n",
    "        'circuit': circuit_name,\n",
    "        'timestamp': timestamp,\n",
    "        'original_shape': list(df.shape),\n",
    "        'engineered_shape': list(df_features.shape),\n",
    "        'features_created': len(df_features.columns) - len(df.columns),\n",
    "        'datasets_created': len(datasets),\n",
    "        'all_features': list(df_features.columns),\n",
    "        'dataset_info': {\n",
    "            set_name: {\n",
    "                'feature_count': len(dataset['features']),\n",
    "                'sample_count': len(dataset['X']),\n",
    "                'features': dataset['features']\n",
    "            } for set_name, dataset in datasets.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = f\"data/preprocessed/preprocessing_summary_{circuit_name}_{timestamp}.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"âœ… Summary: {summary_path}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸŽ‰ PREPROCESSING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"ðŸ Circuit: {circuit_name.upper()} GP\")\n",
    "    print(f\"ðŸ“Š Original: {df.shape} â†’ Engineered: {df_features.shape}\")\n",
    "    print(f\"â­ Features added: {len(df_features.columns) - len(df.columns)}\")\n",
    "    print(f\"ðŸŽ¯ Total features: {len(df_features.columns)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Feature Sets Created:\")\n",
    "    for set_name, dataset in datasets.items():\n",
    "        print(f\"  â€¢ {set_name}: {len(dataset['features'])} features, {len(dataset['X'])} samples\")\n",
    "    \n",
    "    print(f\"\\nðŸ”— NEXT STEPS:\")\n",
    "    print(f\"  1. Notebook 04: Train VAE using engineered dataset\")\n",
    "    print(f\"  2. Notebook 05: Train Bayesian Network with VAE latent features\")\n",
    "    print(f\"  3. Both notebooks will read from: {full_dataset_path}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Key Output Files:\")\n",
    "    print(f\"  ðŸŽ¯ Main Dataset: {full_dataset_path}\")\n",
    "    print(f\"  ðŸ”§ Artifacts: {artifacts_path}\")\n",
    "    print(f\"  ðŸ“‹ Summary: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Ready for VAE and Bayesian Network implementation!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot save - no datasets created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
